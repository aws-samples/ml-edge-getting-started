{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41043ac9",
   "metadata": {},
   "source": [
    "# Yolov4 Pytorch 1.7 for Edge Devices with Amazon SageMaker\n",
    "\n",
    "\n",
    "Amazon SageMaker is a fully managed machine learning service. With SageMaker, data scientists and developers can quickly and easily build and train machine learning models, and then directly deploy them into a production-ready hosted environment. It provides an integrated Jupyter authoring notebook instance for easy access to your data sources for exploration and analysis, so you don't have to manage servers. It also provides common machine learning algorithms that are optimized to run efficiently against extremely large data in a distributed environment. With native support for bring-your-own-algorithms and frameworks, SageMaker offers flexible distributed training options that adjust to your specific workflows.\n",
    "\n",
    "SageMaker also offers capabilities to prepare models for deployment at the edge. [SageMaker Neo](https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html) is a capability of Amazon SageMaker that enables machine learning models to train once and run anywhere in the cloud and at the edge and [Amazon SageMaker Edge Manager](https://docs.aws.amazon.com/sagemaker/latest/dg/edge.html) provides model management for edge devices so you can optimize, secure, monitor, and maintain machine learning models on fleets of edge devices such as smart cameras, robots, personal computers, and mobile devices.\n",
    "\n",
    "\n",
    "In this notebook we'll train a [**Yolov4**](https://github.com/WongKinYiu/PyTorch_YOLOv4) model on Pytorch using Amazon SageMaker to draw bounding boxes around images and then, compile and package it so that it can be deployed on an edge device(in this case, a [Jetson Xavier](https://developer.nvidia.com/jetpack-sdk-441-archive)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf335e7b",
   "metadata": {},
   "source": [
    "## 1) Pre-requisites\n",
    "\n",
    "Let us start with setting up the pre-requisites for this notebook. First, we will sagemaker and other related libs and then set up the role and buckets and some variables. Note that, we are also specifying the size of the image and also the model size taken as Yolov5s where s stand for small. Check out the [github doc of yolov4](https://github.com/WongKinYiu/PyTorch_YOLOv4) to understand how model sizes differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50874435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from sagemaker.pytorch.estimator import PyTorch\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session=sagemaker.Session()\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "img_size=416\n",
    "model_type='tiny' # tiny or ''\n",
    "model_name='yolov4'if model_type=='' else f\"yolov4-{model_type}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8832f5e",
   "metadata": {},
   "source": [
    "## 2) Download a public implementation of Yolov4 for Pytorch (Author: Wong Kin Yiu)\n",
    "\n",
    "Now, we will download the PyTorch implementation of Yolov4 from this [repository](https://github.com/WongKinYiu/PyTorch_YOLOv4) which is authored by Wong Kin Yiu. We will place it in a local directory `yolov4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be78fd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('yolov4'):\n",
    "    !git clone https://github.com/WongKinYiu/PyTorch_YOLOv4 yolov4\n",
    "    !cd yolov4 && git checkout 3c42cbd1b0fa28ad19436d01e0e240404463ff80 && git apply ../mish.patch\n",
    "    !echo 'tensorboard' > yolov4/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d9fad8",
   "metadata": {},
   "source": [
    "## 3.1) Train a new model (Optional)\n",
    "**If you just want to get a pre-trained model, go to section 3.2**\n",
    "\n",
    "### 3.1.1) Prepare a Python script that will be the entrypoint of the training process\n",
    "\n",
    "Now, we will create a training script to train the Yolov5 model. The training script will wrap the original training scripts and expose the parameters to SageMaker Estimator. The script accepts different arguments which will control the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d000c755",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile yolov4/sagemaker_train.py\n",
    "import sys\n",
    "import subprocess\n",
    "## We need to remove smdebug to avoid the Hook bug https://github.com/awslabs/sagemaker-debugger/issues/401\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"smdebug\"])\n",
    "import os\n",
    "import yaml\n",
    "import argparse\n",
    "import torch\n",
    "import shutil\n",
    "import urllib\n",
    "from models.models import Darknet\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('--num-classes', type=int, default=80, help='Number of classes')\n",
    "    parser.add_argument('--img-size', type=int, default=640, help='Size of the image')\n",
    "    parser.add_argument('--epochs', type=int, default=1, help='Number of epochs')\n",
    "    parser.add_argument('--batch-size', type=int, default=16, help='Batch size')\n",
    "    parser.add_argument('--adam', action='store_true', help='use torch.optim.Adam() optimizer')\n",
    "    parser.add_argument('--pretrained', action='store_true', help='use pretrained model')\n",
    "    \n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ[\"SM_MODEL_DIR\"], help='Trained model dir')\n",
    "    parser.add_argument('--train', type=str, default=os.environ[\"SM_CHANNEL_TRAIN\"], help='Train path')\n",
    "    parser.add_argument('--train-suffix', type=str, default='', help='Train path suffix')\n",
    "    parser.add_argument('--validation', type=str, default=os.environ[\"SM_CHANNEL_VALIDATION\"], help='Validation path')\n",
    "    parser.add_argument('--validation-suffix', type=str, default='', help='Validation path suffix')\n",
    "    \n",
    "    parser.add_argument('--model-type', type=str, choices=['', 'tiny'], default=\"\", help='Model type')\n",
    "    \n",
    "    # hyperparameters\n",
    "    with open('data/hyp.scratch.yaml', 'r') as f:\n",
    "        hyperparams = yaml.load(f, Loader=yaml.FullLoader)    \n",
    "    for k,v in hyperparams.items():\n",
    "        parser.add_argument(f\"--{k.replace('_', '-')}\", type=float, default=v)\n",
    "    \n",
    "    args,unknown = parser.parse_known_args()\n",
    "    \n",
    "    base_path=os.path.dirname(__file__)\n",
    "    project_dir = os.environ[\"SM_OUTPUT_DATA_DIR\"]\n",
    "\n",
    "    # prepare the hyperparameters metadat\n",
    "    with open(os.path.join(base_path,'data', 'hyp.custom.yaml'), 'w' ) as y:\n",
    "        y.write(yaml.dump({h:vars(args)[h] for h in hyperparams.keys()}))\n",
    "\n",
    "    # prepare the training data metadata\n",
    "    with open(os.path.join(base_path,'data', 'custom.yaml'), 'w') as y:\n",
    "        y.write(yaml.dump({            \n",
    "            'names': [f'class_{i}' for i in range(args.num_classes)],\n",
    "            'train': os.path.join(args.train, args.train_suffix),\n",
    "            'val': os.path.join(args.validation, args.validation_suffix),\n",
    "            'nc': args.num_classes\n",
    "        }))\n",
    "    model_name = \"yolov4\" if len(args.model_type) == 0 else f\"yolov4-{args.model_type}\"\n",
    "    # run the training script\n",
    "    weights_file=''\n",
    "    if args.pretrained:\n",
    "        weights_file = f'weights/{model_name}.weights'\n",
    "        urllib.request.urlretrieve(\n",
    "            f'https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/{model_name}.weights',\n",
    "            weights_file\n",
    "        )\n",
    "\n",
    "    train_cmd = [\n",
    "        sys.executable, os.path.join(base_path,'train.py'),\n",
    "        \"--data\", \"custom.yaml\",\n",
    "        \"--hyp\", \"hyp.custom.yaml\",\n",
    "        \"--cfg\", f\"cfg/{model_name}.cfg\",\n",
    "        \"--img\", str(args.img_size),\n",
    "        \"--batch\", str(args.batch_size),\n",
    "        \"--epochs\", str(args.epochs),\n",
    "        \"--logdir\", project_dir,\n",
    "        \"--weights\", weights_file\n",
    "    ]\n",
    "    if args.adam: train_cmd.append(\"--adam\")\n",
    "    subprocess.check_call(train_cmd)\n",
    "        \n",
    "    # tracing and saving the model\n",
    "    inp = torch.rand(1, 3, args.img_size, args.img_size).cpu()\n",
    "    ckpt = torch.load(os.path.join(project_dir, 'exp0', 'weights', 'best.pt'), map_location='cpu')\n",
    "    model = Darknet(f\"cfg/{model_name}.cfg\").cpu()\n",
    "    # do not invoke .eval(). we don't need the detection layer\n",
    "    model.load_state_dict(ckpt['model'], strict=False)\n",
    "    p = model(inp)\n",
    "    model_trace = torch.jit.trace(model, inp)\n",
    "    model_trace.save(os.path.join(args.model_dir, 'model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25bdad8",
   "metadata": {},
   "source": [
    "### 3.1.2) Preparing the dataset\n",
    "\n",
    "Here we'll download a sample dataset [coco128](https://github.com/ultralytics/yolov5/releases/download/v1.0/coco128.zip). We can also replace this step with any other dataset. \n",
    "\n",
    "Just take a look on the labels format and create your dataset scructure following the same standard (COCO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df639c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('coco128'):\n",
    "    !wget -q https://github.com/ultralytics/yolov5/releases/download/v1.0/coco128.zip\n",
    "    !unzip -q coco128.zip && rm -f coco128.zip\n",
    "print('BBoxes annotation')\n",
    "print('class x_center y_center width height')\n",
    "!head coco128/labels/train2017/000000000009.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2fff9c",
   "metadata": {},
   "source": [
    "### 3.1.3) Upload the dataset to S3\n",
    "\n",
    "Once the dataset has been downloaded locally, we'll upload the dataset to an S3 bucket created earlier. We are setting up the training and validation dataset s3 locations here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9c7fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix='data/coco128'\n",
    "!rm -f coco128/labels/train2017.cache\n",
    "train_path = sagemaker_session.upload_data('coco128', key_prefix=f'{prefix}/train')\n",
    "val_path = sagemaker_session.upload_data('coco128', key_prefix=f'{prefix}/val')\n",
    "print(train_path, val_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9249a2e8",
   "metadata": {},
   "source": [
    "### 3.1.4) Prepare the SageMaker Estimator to train the model\n",
    "\n",
    "Now it's time to create an Estimater and train the model with the training script created in earlier step. We are using Pytorch estimator and supplying other arguments in the estimator. Note that we are supplying the `source_dir` so that sagemaker can pick up the training script and other related files from there. Once the estimator is ready, we start the training using the `.fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded6992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    'sagemaker_train.py',\n",
    "    source_dir='yolov4',\n",
    "    framework_version='1.7',\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    instance_type='ml.p3.2xlarge',    \n",
    "    instance_count=1,\n",
    "    py_version='py3', \n",
    "    hyperparameters={\n",
    "        'epochs': 20, # at least 2 epochs\n",
    "        'batch-size': 8,\n",
    "        'lr0': 0.0001,\n",
    "        \n",
    "        'pretrained': True, # transfer learning\n",
    "        'num-classes': 80,\n",
    "        'img-size': img_size,\n",
    "        'model-type': model_type,\n",
    "        \n",
    "        # the final path needs to point to the images dir\n",
    "        'train-suffix': 'images/train2017',\n",
    "        'validation-suffix': 'images/train2017'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c260033",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator.fit({'train': train_path, 'validation': val_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc16b603",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_uri=f'{estimator.output_path}{estimator.latest_training_job.name}/output/model.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70a45f2",
   "metadata": {},
   "source": [
    "## 3.2) Pre-trained model Yolov4-Tiny (Optional)\n",
    "**If you don't want to train a new model and just want to get a pre-trained model, run this section**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468f4386",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download the weights\n",
    "import os\n",
    "from http.cookiejar import CookieJar\n",
    "import urllib\n",
    "\n",
    "if not os.path.isfile('yolov4-tiny.pt'):\n",
    "    cookie_processor = urllib.request.HTTPCookieProcessor(CookieJar())\n",
    "    opener = urllib.request.build_opener(cookie_processor)  \n",
    "    urllib.request.install_opener(opener)\n",
    "\n",
    "    fileid=\"1aQKcCvTAl1uOWzzHVE9Z8Ixgikc3AuYQ\"\n",
    "    filename=\"yolov4-tiny.pt\"\n",
    "    base_url=f\"https://drive.google.com/uc?export=download&id={fileid}\"\n",
    "    opener.open(base_url) # create session\n",
    "    resp = opener.open(f\"{base_url}&confirm=\") # download weights\n",
    "    data = resp.read() # save weights to disk\n",
    "    open('yolov4-tiny.pt', 'wb').write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87fc96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the weights, trace and upload (to s3) the model\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import tarfile\n",
    "sys.path.insert(0, 'yolov4')\n",
    "from models.models import Darknet\n",
    "\n",
    "if not os.path.isfile('model.tar.gz'):\n",
    "    model = Darknet('yolov4/cfg/yolov4-tiny.cfg', 416).cpu()\n",
    "    model.load_state_dict(torch.load('yolov4-tiny.pt')['model'], strict=False)\n",
    "    inp = torch.rand(1,3,416,416).cpu()\n",
    "    _ = model(inp)\n",
    "    model_trace = torch.jit.trace(model, inp)\n",
    "    model_trace.save('model.pth')\n",
    "\n",
    "    with tarfile.open('model.tar.gz', 'w:gz' ) as tar:\n",
    "        tar.add(\"model.pth\")\n",
    "        tar.list()\n",
    "\n",
    "s3_uri = sagemaker_session.upload_data('model.tar.gz', key_prefix=f'{model_name}/model')\n",
    "print(s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2cf785",
   "metadata": {},
   "source": [
    "## 4) Compile your trained model for the edge device\n",
    "\n",
    "Once the model has been traied, we need to compile the model using SageMaker Neo. This step is needed to compile the model for the specific hardware on which this model will be deployed. \n",
    "\n",
    "In this notebook, we will compile a model for [Jetson Xavier Jetpack 4.4.1](https://developer.nvidia.com/jetpack-sdk-441-archive). \n",
    "\n",
    "In case, you want to compile for a different hardware platform, just change the parameters bellow to adjust the target to your own edge device. Also, note, that if you dont have GPU available on the hardware device, then you can comment the `Accelerator` key:value in the `OutputConfig`.\n",
    "\n",
    "The below cell also calls the `describe_compilation_job` API in a loop to wait for the compilation job to complete. In actual applications, it is advisable to setup a cloudwatch event which can notify OR execute the next steps once the compilation job is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c023e4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import boto3\n",
    "sm_client = boto3.client('sagemaker')\n",
    "compilation_job_name = f'{model_name}-pytorch-{int(time.time()*1000)}'\n",
    "sm_client.create_compilation_job(\n",
    "    CompilationJobName=compilation_job_name,\n",
    "    RoleArn=role,\n",
    "    InputConfig={\n",
    "        'S3Uri': s3_uri,\n",
    "        'DataInputConfig': f'{{\"input\": [1,3,{img_size},{img_size}]}}',\n",
    "        'Framework': 'PYTORCH'\n",
    "    },\n",
    "    OutputConfig={\n",
    "        'S3OutputLocation': f's3://{sagemaker_session.default_bucket()}/{model_name}-pytorch/optimized/',\n",
    "        'TargetPlatform': { \n",
    "            'Os': 'LINUX', \n",
    "            'Arch': 'ARM64', # change this to X86_64 if you need\n",
    "            'Accelerator': 'NVIDIA'  # comment this if you don't have an Nvidia GPU\n",
    "        },\n",
    "        # Comment or change the following line depending on your edge device\n",
    "        # Jetson Xavier: sm_72; Jetson Nano: sm_53\n",
    "        'CompilerOptions': '{\"trt-ver\": \"7.1.3\", \"cuda-ver\": \"10.2\", \"gpu-code\": \"sm_53\"}' # Jetpack 4.4.1\n",
    "    },\n",
    "    StoppingCondition={ 'MaxRuntimeInSeconds': 900 }\n",
    ")\n",
    "while True:\n",
    "    resp = sm_client.describe_compilation_job(CompilationJobName=compilation_job_name)    \n",
    "    if resp['CompilationJobStatus'] in ['STARTING', 'INPROGRESS']:\n",
    "        print('Running...')\n",
    "    else:\n",
    "        print(resp['CompilationJobStatus'], compilation_job_name)\n",
    "        break\n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bfd8e9",
   "metadata": {},
   "source": [
    "## 5) Create a SageMaker Edge Manager packaging job\n",
    "\n",
    "Once the model has been compiled, it is time to create an edge manager packaging job. Packaging job take SageMaker Neoâ€“compiled models and make any changes necessary to deploy the model with the inference engine, Edge Manager agent.\n",
    "\n",
    "We need to provide the name used for the Neo compilation job, a name for the packaging job, a role ARN, a name for the model, a model version, and the Amazon S3 bucket URI for the output of the packaging job. Note that Edge Manager packaging job names are case-sensitive.\n",
    "\n",
    "\n",
    "The below cell also calls the `describe_edge_packaging_job` API in a loop to wait for the packaging job to complete. In actual applications, it is advisable to setup a cloudwatch event which can notify OR execute the next steps once the compilation job is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea04ee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "model_version = '1.0'\n",
    "edge_packaging_job_name=f'{model_name}-pytorch-{int(time.time()*1000)}'\n",
    "resp = sm_client.create_edge_packaging_job(\n",
    "    EdgePackagingJobName=edge_packaging_job_name,\n",
    "    CompilationJobName=compilation_job_name,\n",
    "    ModelName=model_name,\n",
    "    ModelVersion=model_version,\n",
    "    RoleArn=role,\n",
    "    OutputConfig={\n",
    "        'S3OutputLocation': f's3://{bucket_name}/{model_name}'\n",
    "    }\n",
    ")\n",
    "while True:\n",
    "    resp = sm_client.describe_edge_packaging_job(EdgePackagingJobName=edge_packaging_job_name)    \n",
    "    if resp['EdgePackagingJobStatus'] in ['STARTING', 'INPROGRESS']:\n",
    "        print('Running...')\n",
    "    else:\n",
    "        print(resp['EdgePackagingJobStatus'], compilation_job_name)\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57f9a75",
   "metadata": {},
   "source": [
    "## 6) Pre processing + Post processing code\n",
    "After compiling your model, it's time to prepare the application that will use it. In the image bellow you can see the operators that are used by the last layer **YOLOLayer**. When in evaluation mode, this layer applies some operations to merge the two outputs of the network and prepare the predictions for the **Non Maximum Suppression**. In training model, you just have the two raw outputs. \n",
    "\n",
    "Given we're using a pruned version (training mode) of the network, you need to apply some post processing code to your predictions.\n",
    "\n",
    "<table style=\"border: 1px solid black; border-collapse: collapse;\" border=3 cellpadding=0 cellspacing=0>\n",
    "    <tr style=\"border: 1px solid black;\">\n",
    "        <td style=\"text-align: center; border-right: 1px solid;\" align=\"center\"><b>WITH DETECTION (evaluation mode)</b></td>\n",
    "        <td style=\"text-align: center;\" align=\"center\"><b>NO DETECTION (training mode)</b></td>\n",
    "    </tr>    \n",
    "    <tr style=\"border: 1px solid black; border-right: 1px solid;\">\n",
    "        <td width=\"75%\" style=\"border-right: 1px solid;\">\n",
    "            <img src=\"imgs/yolov4_detection.png\"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"imgs/yolov4_no_detection.png\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "All the operations required by this process can be found in the script **[Utils](utils.py)**.\n",
    "#### WARNING: run the next cell and copy the output to your **utils.py** before running the application. It is necessary to adjust the anchors and scores for the correct yolov4 version\n",
    "\n",
    "\n",
    "Here it is an example of how to use the code:\n",
    "```Python\n",
    "import utils\n",
    "\n",
    "### your code here\n",
    "### def predict(model, x):...\n",
    "\n",
    "confidence_treshold=0.1\n",
    "# Read the image using OpenCV\n",
    "img = cv2.imread('dog.jpg')\n",
    "# Convert the image to the expected network input\n",
    "x = utils.preprocess_img(img, img_size=416)\n",
    "# Run the model and get the predictions\n",
    "preds = predict(model, x)\n",
    "# Convert the predictions into Detections(bounding boxes, scores and class ids)\n",
    "detections = utils.detect(preds, 0.25, 0.5, True)\n",
    "# Iterate over the detections and do what you need to do\n",
    "for top_left_corner,bottom_right_corner, conf, class_id in detections:\n",
    "    if confidence_treshold < 0.1: continue\n",
    "    print( f\"bbox: {top_left_corner},{bottom_right_corner}, score: {conf}, class_id: {class_id}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4834909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code parses the .cfg file of the version you're using and\n",
    "## Prints the correct anchors/stride. Copy the output to the 'utils.py' file used by your application\n",
    "import numpy as np\n",
    "\n",
    "cfg_filename=f'yolov4/cfg/{model_name}.cfg'\n",
    "yolo_layer=False\n",
    "yolo_layers = []\n",
    "\n",
    "for i in open(cfg_filename, 'r').readlines(): # read the .cfg file\n",
    "    i = i.strip()\n",
    "    if len(i) == 0 or i.startswith('#'): continue # ignore empty lines and comments\n",
    "    elif i.startswith('[') and i.endswith(']'): # header\n",
    "        if i.lower().replace(' ', '') == '[yolo]':\n",
    "            yolo_layer = True # yolo layer\n",
    "            yolo_layers.append({})\n",
    "    elif yolo_layer: # properties of the layer\n",
    "        k,v = [a.strip() for a in i.split('=')] # split line into key, value\n",
    "        if k == 'anchors': # parse anchors\n",
    "            anchors = np.array([int(j.strip()) for j in v.split(',')])\n",
    "            yolo_layers[-1]['anchors'] = anchors.reshape((len(anchors)//2, 2))\n",
    "        elif k == 'mask': # parse mask\n",
    "            yolo_layers[-1]['mask'] = np.array([int(j.strip()) for j in v.split(',')])\n",
    "\n",
    "stride = [8, 16, 32, 64, 128]  # P3, P4, P5, P6, P7 strides\n",
    "if model_type == 'tiny':  # P5, P4, P3 strides\n",
    "    stride = [32, 16, 8]\n",
    "\n",
    "print(\"##### Copy the following lines to your util.py\")\n",
    "print(f\"anchors = {[l['anchors'][l['mask']].tolist() for l in yolo_layers]}\")\n",
    "print(f\"stride = {stride}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f9f61a",
   "metadata": {},
   "source": [
    "### Done !!\n",
    "\n",
    "And we are done with all the steps needed to prepare the model for deploying to edge. The model package is avaialble in S3 and can be taken from there to deploy it to edge device. Now you need to move over to your edge device and download and setup edge manager agent(runtime), model and other related artifacts on the device. Please check out the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/edge.html) for detailed steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
